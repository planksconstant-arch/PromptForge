/**
 * Gemini Service (API Client)
 * Connects to Python Backend
 */

const API_URL = 'http://localhost:8000';

export const optimizePrompt = async (
  structuredPrompt: string,
  originalUserPrompt: string,
  model: string,
  goal: string,
  userResources: string,
  currentSkills: string,
  timeCommitment: string
): Promise<any> => {
  // We reuse the /optimizer/optimize endpoint or generalized LLM endpoint
  // Mapping complex TS arguments to a simpler prompt for the backend
  const prompt = `Optimize this: ${structuredPrompt}\nOriginal: ${originalUserPrompt}\nGoal: ${goal}`;

  const response = await fetch(`${API_URL}/llm/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ prompt, options: { model } })
  });

  if (!response.ok) throw new Error("Optimization failed");

  const data = await response.json();
  return {
    prompt: data.text,
    reasoning: "Generated by Python Backend",
    critique: null
  };
};